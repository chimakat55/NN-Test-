# -*- coding: utf-8 -*-
"""Exercise 5 DL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SLlYJHV38rtgUGTh5QxHT1y6aOkNAQ5I
"""

# Imports
import numpy as np

# Each row is a training example, each column is a feature  [X1, X2, X3]
X=np.array(([1],[0],[0],[0]), dtype=float)
y=np.array(([1],[0],[0]), dtype=float)

# Define useful functions    

# Activation function
def sigmoid(t):
    return 1/(1+np.exp(-t))

# Derivative of sigmoid
def sigmoid_derivative(p):
    return p * (1 - p)

# sofmax activation
def softmax(xs):
    return np.exp(xs) / sum(np.exp(xs))
 

# derivative of softmax
def softmax_derivative(pred):
    return pred * (1 -(1 * pred).sum(axis=1)[:,None])

def cross_entropy(X,y):
    X = X.clip(min=1e-8,max=None)
    # print('\n\nCE: ', (np.where(y==1,-np.log(X), 0)).sum(axis=1))
    return (np.where(y==1,-np.log(X), 0)).sum(axis=1)

def cross_entropy_derivative(X,y):
    X = X.clip(min=1e-8,max=None)
    # print('\n\nCED: ', np.where(y==1,-1/X, 0))
    return np.where(y==1,-1/X, 0)

# Class definition
class NeuralNetwork:
    def __init__(self, x,y):
        self.input = x
        self.weights1= np.array([[1, 1, 0.5, 0], [0, 1, 0, 1]]) # considering we have 4 nodes in the hidden layer
        self.weights2 = np.array([[-1, -1], [-1, 0]])
        self.weights3 = np.array([[0, 0], [0.5, 0], [0,0]])
        self.y = y
        self.output = np.zeros(y.shape)
        
    def feedforward(self):
        self.layer1 = sigmoid(np.dot(self.weights1, self.input))
        self.layer2 = sigmoid(np.dot(self.weights2, self.layer1))
        self.layer3 = softmax(np.dot(self.weights3, self.layer2))
        return self.layer3

## IGNORE: Deriving the forward and backpropagated weights alone ##

input = np.array(([1],[0],[0],[0]))
weights1= np.array([[1, 1, 0.5, 0], [0, 1, 0, 1]]) # considering we have 4 nodes in the hidden layer
weights2 = np.array([[-1, -1], [-1, 0]])
weights3 = np.array([[0, 0], [0.5, 0], [0,0]])
y = np.array(([1],[0],[0]), dtype=float)
output = np.zeros(y.shape)
        
layer1 = sigmoid(np.dot(weights1, input))
layer2 = sigmoid(np.dot(weights2, layer1))
layer3 = softmax(np.dot(weights3, layer2))


    
dloss = cross_entropy_derivative(output, y)  # 2*(self.y - self.pred)
d_weights3 = np.dot(2*(y -output)*softmax_derivative(output), layer1.T)
d_weights2 = np.dot(layer1.T, np.dot(2*(y -output)*softmax_derivative(output), weights3.T)*sigmoid_derivative(layer2))

d_weights2 = np.dot(layer1.T, np.dot(dloss.dot((softmax_derivative(output).T)), weights3)*sigmoid_derivative(layer2))

print(softmax_derivative(output).shape)

## Alternative backprob##

def backprop(self):
        dloss = cross_entropy_derivative(self.output, self.y)  # 2*(self.y - self.pred)
        d_weights3 = np.dot(self.layer2.T, dloss*softmax_derivative(self.output))
        d_weights2 = np.dot(self.layer1.T, np.dot(dloss*softmax_derivative(self.output), self.weights3.T)*sigmoid_derivative(self.layer2))
        d_weights1 = np.dot(self.input.T, np.dot(dloss*softmax_derivative(self.output), self.weights2.T)*sigmoid_derivative(self.layer1))
        
        self.weights1 += d_weights1
        self.weights2 += d_weights2
        self.weights3 += d_weights3

def train(self, X, y):
        self.output = self.feedforward()
        self.backprop()

def backprop(self):
        dloss = cross_entropy_derivative(self.output, self.y)
        d_weights3 = np.dot(self.layer2.T, dloss*softmax_derivative(self.output))
        d_weights2 = np.dot(self.layer1.T, np.dot(dloss*sigmoid_derivative(self.output), self.weights3.T)*sigmoid_derivative(self.layer2))
        d_weights1 = np.dot(self.input.T, np.dot(dloss*sigmoid_derivative(self.output), self.weights2.T)*sigmoid_derivative(self.layer1))
    
        self.weights1 += d_weights1
        self.weights2 += d_weights2
        self.weights3 += d_weights3

def train(self, X, y):
        self.output = self.feedforward()
        self.backprop()

NN = NeuralNetwork(X,y)
for i in range(10): # trains the NN 1,000 times
    if i % 1 ==0: 
        print ("for iteration # " + str(i) + "\n")
        print ("Input : \n" + str(X))
        print ("Actual Output: \n" + str(y))
        print ("Predicted Output: \n" + str(NN.feedforward()))
        print ("Loss: \n" + str(cross_entropy()) 
        print ("\n")
  
NN.train(X, y)


